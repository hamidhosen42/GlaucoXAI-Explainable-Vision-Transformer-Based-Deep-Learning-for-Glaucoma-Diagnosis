{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":18613,"sourceType":"datasetVersion","datasetId":5839},{"sourceId":23812,"sourceType":"datasetVersion","datasetId":17810},{"sourceId":852947,"sourceType":"datasetVersion","datasetId":451612},{"sourceId":856366,"sourceType":"datasetVersion","datasetId":453636},{"sourceId":896894,"sourceType":"datasetVersion","datasetId":479737},{"sourceId":898766,"sourceType":"datasetVersion","datasetId":480988},{"sourceId":900014,"sourceType":"datasetVersion","datasetId":481794},{"sourceId":955859,"sourceType":"datasetVersion","datasetId":519658},{"sourceId":955864,"sourceType":"datasetVersion","datasetId":519660},{"sourceId":958085,"sourceType":"datasetVersion","datasetId":521486},{"sourceId":958593,"sourceType":"datasetVersion","datasetId":521842},{"sourceId":982278,"sourceType":"datasetVersion","datasetId":537357},{"sourceId":986804,"sourceType":"datasetVersion","datasetId":540281},{"sourceId":1019787,"sourceType":"datasetVersion","datasetId":560919},{"sourceId":7000210,"sourceType":"datasetVersion","datasetId":4024114}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"colab":{"provenance":[]}},"nbformat_minor":0,"nbformat":4,"cells":[{"cell_type":"markdown","source":["In this notebook, I am going to try and create a high-performance model for pneumonia diagnosis based on X-Rays.\n","\n","Let's get started by installing and importing the necessary libraries."],"metadata":{"id":"TJIiiGqlwUGi"}},{"cell_type":"code","source":["# Install the imutils library for image preprocessing\n","!pip install imutils"],"metadata":{"execution":{"iopub.status.busy":"2024-03-24T13:35:51.516933Z","iopub.execute_input":"2024-03-24T13:35:51.517290Z","iopub.status.idle":"2024-03-24T13:36:07.278047Z","shell.execute_reply.started":"2024-03-24T13:35:51.517259Z","shell.execute_reply":"2024-03-24T13:36:07.277109Z"},"trusted":true,"id":"kfrhI4tLwUGj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Imorting necessary libraries\n","from imutils import paths\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","\n","import tensorflow as tf\n","from tensorflow.keras.preprocessing.image import load_img, img_to_array, array_to_img\n","from tensorflow.keras import Model, Sequential\n","from tensorflow.keras import layers\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","import tensorflow.keras.metrics as metrics\n","\n","# Setting some variables for plotting via matplotlib\n","plt.rcParams[\"figure.figsize\"] = (10, 10)\n","plt.rc('xtick', labelsize=15)\n","plt.rc('ytick', labelsize=15)\n","plt.rc('lines', linewidth=3)\n","plt.rc('font', size=15)"],"metadata":{"id":"9ctT-ggsCYNk","execution":{"iopub.status.busy":"2024-03-24T13:36:21.819483Z","iopub.execute_input":"2024-03-24T13:36:21.820395Z","iopub.status.idle":"2024-03-24T13:36:34.686782Z","shell.execute_reply.started":"2024-03-24T13:36:21.820344Z","shell.execute_reply":"2024-03-24T13:36:34.685961Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["In the code cell below, I'm fixing random seeds for reproducibility, although note that the apparent non-determinism of TensorFlow doesn't allow full reproducibility."],"metadata":{"id":"YYs01k8mwUGl"}},{"cell_type":"code","source":["import random\n","\n","random.seed(10)\n","np.random.seed(10)\n","tf.random.set_seed(10)"],"metadata":{"execution":{"iopub.status.busy":"2024-03-24T13:38:52.257426Z","iopub.execute_input":"2024-03-24T13:38:52.258227Z","iopub.status.idle":"2024-03-24T13:38:52.263794Z","shell.execute_reply.started":"2024-03-24T13:38:52.258190Z","shell.execute_reply":"2024-03-24T13:38:52.262707Z"},"trusted":true,"id":"vLSbbWc0wUGl"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["In the following couple of cells, we will be reading image jpeg files from the disk. The images are separated into directories based on their diagnosis (NORMAL and PNEUMONIA) and purpose (test, train, val). Thanks to the separation via diagnosis, I can easily create corresponding labels for each category.\n","\n","Note that the PNEUMONIA X-Rays in this dataset are also divided into bacterial and viral pneumonia (which can be seen from their filenames). However, in this particular notebook, I will only be reviewing binary classification - NORMAL vs PNEUMONIA.\n","\n","I will not be loading the images into memory right now - instead, this will be done by ImageDataGenerator objects during training. This is because I wanted to feed 512 x 512 x 1 images, which was too big for loading the entire dataset into memory. So right now, I am only declaring lists containing image paths, not image arrays.\n","\n","The labels are strings as well since this is what's required by the *flow_from_dataframe* method of the *ImageDataGenerator* class that I will be using later."],"metadata":{"id":"CmFk5kW7wUGl"}},{"cell_type":"code","source":["# Find and load normal image directories into a list\n","directory_test = '/kaggle/input/fundus-pytorch/test/0/'\n","directory_train = '/kaggle/input/fundus-pytorch/train/0/'\n","directory_val = '/kaggle/input/fundus-pytorch/val/0/'\n","image_paths_norm = sorted(list(paths.list_images(directory_test))) + \\\n","                   sorted(list(paths.list_images(directory_train))) + \\\n","                   sorted(list(paths.list_images(directory_val)))\n","\n","# Declare a normal label list\n","labels_norm = ['No Glaucoma'] * len(image_paths_norm)"],"metadata":{"id":"XyF1eyCmD9uO","execution":{"iopub.status.busy":"2024-03-24T13:40:25.869690Z","iopub.execute_input":"2024-03-24T13:40:25.870404Z","iopub.status.idle":"2024-03-24T13:40:38.402085Z","shell.execute_reply.started":"2024-03-24T13:40:25.870360Z","shell.execute_reply":"2024-03-24T13:40:38.401247Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Find and load pneumonia image directories into a list\n","directory_test_pn = '/kaggle/input/fundus-pytorch/test/1/'\n","directory_train_pn = '/kaggle/input/fundus-pytorch/train/1/'\n","directory_val_pn = '/kaggle/input/fundus-pytorch/val/1/'\n","image_paths_pn = sorted(list(paths.list_images(directory_test_pn))) + \\\n","                 sorted(list(paths.list_images(directory_train_pn))) + \\\n","                 sorted(list(paths.list_images(directory_val_pn)))\n","\n","# Declare a pneumonia label list for binary\n","labels_pn = ['Glaucoma'] * len(image_paths_pn)\n","\n","# Merge the image and label lists together and turn them into NumPy arrays\n","image_paths = np.array(image_paths_norm + image_paths_pn)\n","labels = np.concatenate((labels_norm, labels_pn))"],"metadata":{"execution":{"iopub.status.busy":"2024-03-24T13:40:43.722344Z","iopub.execute_input":"2024-03-24T13:40:43.723289Z","iopub.status.idle":"2024-03-24T13:40:52.096585Z","shell.execute_reply.started":"2024-03-24T13:40:43.723247Z","shell.execute_reply":"2024-03-24T13:40:52.095776Z"},"trusted":true,"id":"EUkXfz7BwUGm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["It's interesting to see how the diagnoses are distributed in the dataset. Below is a bar plot where we can see that glaucoma samples are predominant in the dataset. the model may need thus should not need any class weight adjustment."],"metadata":{"id":"IO6jPeJxwUGn"}},{"cell_type":"code","source":["# Plot a bar with numbers of normal and glaucoma diagnoses\n","plt.bar([1, 2], height=[len(labels_norm), len(labels_pn)],\n","        tick_label=['({0:d} Normal Samples)'.format(len(labels_norm)),\n","                    '({0:d} Glaucoma Samples)'.format(len(labels_pn))],\n","        color=['m', 'g'])\n","\n","plt.title('Number Of Samples Per Available Label ({0:d} Total Samples)'.\n","          format(len(labels_norm) + len(labels_pn)))\n","\n","plt.show()"],"metadata":{"execution":{"iopub.status.busy":"2024-03-24T13:43:09.489771Z","iopub.execute_input":"2024-03-24T13:43:09.490409Z","iopub.status.idle":"2024-03-24T13:43:09.741223Z","shell.execute_reply.started":"2024-03-24T13:43:09.490355Z","shell.execute_reply":"2024-03-24T13:43:09.740386Z"},"trusted":true,"id":"aT86q3-3wUGn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now, let's plot a couple of images from each diagnosis to see how they look like."],"metadata":{"id":"pZgtAjgowUGn"}},{"cell_type":"code","source":["# Build a 2 x 2 figure\n","rows, columns = 2, 2\n","fig, axes = plt.subplots(rows, columns)\n","\n","# Choose random indices for image selection\n","norm_indices = np.random.choice(len(image_paths_norm), rows * columns)\n","pn_indices = np.random.choice(len(image_paths_pn), rows * columns)\n","\n","# Make a list with images to plot\n","imgs_to_plot_norm = [image_paths_norm[i] for i in norm_indices]\n","imgs_to_plot_pn = [image_paths_pn[i] for i in pn_indices]\n","\n","# Variable used for indending through the image list\n","current_index = 0\n","\n","# Set figure title\n","fig.suptitle('4 Random Normal Samples')\n","\n","# Plot normal images\n","for i in range(rows):\n","    for j in range(columns):\n","        img = load_img(imgs_to_plot_norm[current_index])\n","        axes[i, j].imshow(img)\n","        axes[i, j].set_xticks([])\n","        axes[i, j].set_yticks([])\n","        current_index +=1\n","\n","plt.show()"],"metadata":{"execution":{"iopub.status.busy":"2024-03-24T13:43:24.202650Z","iopub.execute_input":"2024-03-24T13:43:24.203021Z","iopub.status.idle":"2024-03-24T13:43:25.028559Z","shell.execute_reply.started":"2024-03-24T13:43:24.202994Z","shell.execute_reply":"2024-03-24T13:43:25.027703Z"},"trusted":true,"id":"cAfNebjpwUGo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["fig, axes = plt.subplots(rows, columns)\n","\n","current_index = 0\n","\n","# Set figure title\n","fig.suptitle('4 Random Glaucoma Samples')\n","\n","# Plot pneumonia images\n","for i in range(rows):\n","    for j in range(columns):\n","        img = load_img(imgs_to_plot_pn[current_index])\n","        axes[i, j].imshow(img)\n","        axes[i, j].set_xticks([])\n","        axes[i, j].set_yticks([])\n","        current_index +=1\n","\n","plt.show()"],"metadata":{"execution":{"iopub.status.busy":"2024-03-24T13:44:45.951521Z","iopub.execute_input":"2024-03-24T13:44:45.952184Z","iopub.status.idle":"2024-03-24T13:44:46.658277Z","shell.execute_reply.started":"2024-03-24T13:44:45.952151Z","shell.execute_reply":"2024-03-24T13:44:46.657335Z"},"trusted":true,"id":"jMdcluoxwUGo"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Generally, pneumonia manifests itself in alveoli consolidations mainly caused by bacteria and fluid. The differences between healthy and pneumonia X-Rays are pretty clearly visible, though in some images, the differences may not be as definite."],"metadata":{"id":"4EFry6ciwUGo"}},{"cell_type":"markdown","source":["As our next step, we will divive our image paths and labels into training, validation, and test sets. The purpose of these sets is as follows:\n","1. **Training data** will be used to update the weights of the network to achieve high performance.\n","2. **Validation data** will be used to tweak model hyperparameters to achieve low loss, high accuracy, and no overfitting.\n","3. **Test data** will be used for final performance validation and to make sure that the model doesn't overfit to the parameters we choose based on validation performance.\n","\n","Note that I am not using the splits proposed by the original publisher of the data - this is because I have been unable to achieve high performance with it. Maybe it's possible, but I decided not to waste time and split the data anew myself.\n","\n","For splitting, I am using scikit-learn's train_test_split method. Since this method only divides the dataset into training and test sets, I've then further divided the test set into validation and test sets - the former and latter halves of the initial test set respectively. The data distribution is 70% training and 15% validation & test data."],"metadata":{"id":"bTtZvLvKwUGo"}},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split\n","\n","# Divide the image path & label arrays into train & test sets. This is done without loading images to save memory\n","X_train_dir, X_test_dir, y_train, y_test = \\\n","                                           train_test_split(image_paths, labels, test_size=0.3)\n","\n","# Divide the test set into validation (for use during training) and test (for post-training evaulation) sets\n","X_val_dir, y_val = X_test_dir[:len(X_test_dir) // 2], \\\n","                   y_test[:len(y_test) // 2]\n","\n","X_test_dir, y_test = X_test_dir[len(X_test_dir) // 2:], \\\n","                     y_test[len(y_test) // 2:]"],"metadata":{"execution":{"iopub.status.busy":"2024-03-24T13:45:26.884682Z","iopub.execute_input":"2024-03-24T13:45:26.885026Z","iopub.status.idle":"2024-03-24T13:45:27.902201Z","shell.execute_reply.started":"2024-03-24T13:45:26.885001Z","shell.execute_reply":"2024-03-24T13:45:27.901382Z"},"trusted":true,"id":"K-RHhiLKwUGo"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["I will be feeding the data into the model via an *ImageDataGenerator* object. However, because we have image paths rather than image arrays, we can't use the *flow* method. Instead, we have to use the *flow_from_dataframe* method, which requires DataFrames containing image paths under the \"filename\" column and labels under the \"class\" column.\n","\n","Below, we create DataFrames for each of our sets."],"metadata":{"id":"334XOaDUwUGo"}},{"cell_type":"code","source":["# Create DataFrames for the image generator\n","train_df = pd.DataFrame(np.transpose([X_train_dir, y_train]),\n","                        columns=['filename', 'class'])\n","\n","val_df = pd.DataFrame(np.transpose([X_val_dir, y_val]),\n","                      columns=['filename', 'class'])\n","\n","test_df = pd.DataFrame(np.transpose([X_test_dir, y_test]),\n","                       columns=['filename', 'class'])"],"metadata":{"execution":{"iopub.status.busy":"2024-03-24T13:45:54.220009Z","iopub.execute_input":"2024-03-24T13:45:54.220381Z","iopub.status.idle":"2024-03-24T13:45:54.237862Z","shell.execute_reply.started":"2024-03-24T13:45:54.220342Z","shell.execute_reply":"2024-03-24T13:45:54.237101Z"},"trusted":true,"id":"chOREnfGwUGp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Next, we declare ImageDataGenerator objects for batch input along with data augmentation to reduce overfitting. Data augmentation is only done on the training data (via the *datagen* object) - as for the validation and test data, only scaling to [0, 1] was performed (via the *test_datagen* object).\n","\n","For data augmentation, I picked parameters so that the output images would be realistic and similar to what the model would be seeing in real-world use.\n","\n","After declaring generator objects, I declare *DataFrameIterator* objects via the *flow_from_dataframe* method. I passed arguments for:\n","1. Target image size (512 x 512).\n","2. Color mode (grayscale).\n","3. Batch size (64).\n","4. Class mode (binary).\n","5. Shuffling (*True* for training data and *False* for validation & test data).\n","\n","When called, the *DataFrameIterator* will read images from the provided filenames, resize them, convert string labels to binary (\"Pneumonia\" to 1 and \"Normal\" to 0), and return 64 image-label pairs."],"metadata":{"id":"ZxlbylHJwUGp"}},{"cell_type":"code","source":["# Set the batch size for the generator and training\n","BATCH_SIZE = 64\n","\n","# Declare an image generator for image augmentation\n","datagen = ImageDataGenerator(rescale = 1./255,\n","                             zoom_range=0.1,\n","                             height_shift_range=0.05,\n","                             width_shift_range=0.05,\n","                             rotation_range=5)\n","\n","\n","# Declare an image generator for validation & testing without generation\n","test_datagen = ImageDataGenerator(rescale = 1./255)\n","\n","# Declare generators for training, validation, and testing from DataFrames\n","train_gen = datagen.flow_from_dataframe(train_df,\n","                                        target_size=(512, 512),\n","                                        color_mode='grayscale',\n","                                        batch_size=BATCH_SIZE,\n","                                        class_mode='binary',\n","                                        shuffle=True)\n","\n","val_gen = test_datagen.flow_from_dataframe(val_df,\n","                                        target_size=(512, 512),\n","                                        color_mode='grayscale',\n","                                        batch_size=BATCH_SIZE,\n","                                        class_mode='binary',\n","                                        shuffle=False)\n","\n","test_gen = test_datagen.flow_from_dataframe(test_df,\n","                                        target_size=(512, 512),\n","                                        color_mode='grayscale',\n","                                        batch_size=BATCH_SIZE,\n","                                        class_mode='binary',\n","                                        shuffle=False)"],"metadata":{"execution":{"iopub.status.busy":"2024-03-24T13:46:23.309629Z","iopub.execute_input":"2024-03-24T13:46:23.309996Z","iopub.status.idle":"2024-03-24T13:46:51.489888Z","shell.execute_reply.started":"2024-03-24T13:46:23.309969Z","shell.execute_reply":"2024-03-24T13:46:51.488842Z"},"trusted":true,"id":"xFJGqWqawUGp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["For a little added input pipeline efficiency, I've also created *tf.Dataset* objects. I wanted to make use of the *prefetch* method of the *tf.Dataset* class to accelerate training a little.\n","\n","Image reading from the disk is a huge bottleneck for the model, and although my approach isn't the best one, it accelerates each epoch by about 20-30 seconds."],"metadata":{"id":"0VvYqLrlwUGp"}},{"cell_type":"code","source":["# Declare TensorFlow Datasets for more efficient training\n","train_data = tf.data.Dataset.from_generator(lambda: train_gen,\n","                                            output_types=(tf.float32, tf.int32),\n","                                            output_shapes=([None, 512, 512, 1], [None, ]))\n","\n","val_data = tf.data.Dataset.from_generator(lambda: val_gen,\n","                                          output_types=(tf.float32, tf.int32),\n","                                          output_shapes=([None, 512, 512, 1], [None, ]))\n","\n","test_data = tf.data.Dataset.from_generator(lambda: test_gen,\n","                                           output_types=(tf.float32, tf.int32),\n","                                           output_shapes=([None, 512, 512, 1], [None, ]))"],"metadata":{"execution":{"iopub.status.busy":"2024-03-24T13:47:25.034873Z","iopub.execute_input":"2024-03-24T13:47:25.035474Z","iopub.status.idle":"2024-03-24T13:47:25.743399Z","shell.execute_reply.started":"2024-03-24T13:47:25.035442Z","shell.execute_reply":"2024-03-24T13:47:25.742439Z"},"trusted":true,"id":"cm9dKNukwUGp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now, let's see how our training generator works in terms of data augmentation. Let's load a couple of images from the disk, pass them to the *flow* method of the generator (this method acccepts image arrays), and plot the base and augmented images in RGB.\n","\n","As you'll see, the augmented images look pretty decently and are realistic enough for training."],"metadata":{"id":"JQ3KMltCwUGp"}},{"cell_type":"code","source":["images_to_augment = []\n","\n","for image_path in image_paths[:4]:\n","    image = load_img(image_path, target_size=(512, 512))\n","    image = img_to_array(image)\n","    images_to_augment.append(image)\n","\n","images_to_augment = np.array(images_to_augment)\n","\n","images_augmented = next(datagen.flow(x=images_to_augment,\n","                                batch_size=10,\n","                                shuffle=False))"],"metadata":{"execution":{"iopub.status.busy":"2024-03-24T13:47:38.565584Z","iopub.execute_input":"2024-03-24T13:47:38.565929Z","iopub.status.idle":"2024-03-24T13:47:38.878050Z","shell.execute_reply.started":"2024-03-24T13:47:38.565904Z","shell.execute_reply":"2024-03-24T13:47:38.877256Z"},"trusted":true,"id":"Z64GGcEFwUGp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from tensorflow.keras.preprocessing.image import array_to_img\n","\n","fig, axes = plt.subplots(2, 2)\n","\n","for i in range(2):\n","    axes[i, 0].imshow(array_to_img(images_to_augment[i]),\n","                      interpolation='nearest')\n","\n","    axes[i, 1].imshow(array_to_img(images_augmented[i]),\n","                      interpolation='nearest')\n","\n","    axes[i, 0].set_xticks([])\n","    axes[i, 1].set_xticks([])\n","\n","    axes[i, 0].set_yticks([])\n","    axes[i, 1].set_yticks([])\n","\n","columns = ['Base Image', 'Augmented Image']\n","for ax, column in zip(axes[0], columns):\n","    ax.set_title(column)\n","\n","plt.show()"],"metadata":{"execution":{"iopub.status.busy":"2024-03-24T13:47:45.888865Z","iopub.execute_input":"2024-03-24T13:47:45.889581Z","iopub.status.idle":"2024-03-24T13:47:46.502792Z","shell.execute_reply.started":"2024-03-24T13:47:45.889552Z","shell.execute_reply":"2024-03-24T13:47:46.501826Z"},"trusted":true,"id":"x4gW0QXtwUGq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["And below is how I used the *prefetch* method of the *tf.Dataset* class that I talked about."],"metadata":{"id":"hAu4he4AwUGq"}},{"cell_type":"code","source":["def feed_data(dataset):\n","    \"\"\"Feed data to a model with prefetching\n","\n","    Arguments:\n","        dataset (tf.Dataset): A dataset that to be fed to the model\n","\n","    Returns:\n","        dataset (tf.Dataset): A prefetched dataset\n","    \"\"\"\n","\n","    dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n","\n","    return dataset"],"metadata":{"execution":{"iopub.status.busy":"2024-03-24T13:48:07.497238Z","iopub.execute_input":"2024-03-24T13:48:07.497590Z","iopub.status.idle":"2024-03-24T13:48:07.502802Z","shell.execute_reply.started":"2024-03-24T13:48:07.497564Z","shell.execute_reply":"2024-03-24T13:48:07.501879Z"},"trusted":true,"id":"mN7yLj8bwUGq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Finally, we've reached model creation. The model architecture that you see below is the final architecture that I chose after some testing. I've saved the results of my test runs in csv files, and we'll be plotting them later so that you see how my model selection went.\n","\n","Now, let's define the model and then plot its summary along with the model architecture."],"metadata":{"id":"l5KHwpPLwUGq"}},{"cell_type":"code","source":["# Define the CNN Keras model\n","def create_model():\n","    \"\"\"\n","    Create a model\n","\n","    Returns:\n","        model (tf.keras.Model): An instance of Model\n","    \"\"\"\n","\n","    # Model input\n","    input_layer = layers.Input(shape=(512, 512, 1), name='input')\n","\n","    # First block\n","    x = layers.Conv2D(filters=64, kernel_size=3,\n","                      activation='relu', padding='same',\n","                      name='conv2d_1')(input_layer)\n","    x = layers.MaxPool2D(pool_size=2, name='maxpool2d_1')(x)\n","    x = layers.Dropout(0.1, name='dropout_1')(x)\n","\n","    # Second block\n","    x = layers.Conv2D(filters=96, kernel_size=3,\n","                      activation='relu', padding='same',\n","                      name='conv2d_2')(x)\n","    x = layers.MaxPool2D(pool_size=2, name='maxpool2d_2')(x)\n","    x = layers.Dropout(0.1, name='dropout_2')(x)\n","\n","    # Third block\n","    x = layers.Conv2D(filters=128, kernel_size=3,\n","                      activation='relu', padding='same',\n","                      name='conv2d_3')(x)\n","    x = layers.MaxPool2D(pool_size=2, name='maxpool2d_3')(x)\n","    x = layers.Dropout(0.1, name='dropout_3')(x)\n","\n","    # Fourth block\n","    x = layers.Conv2D(filters=160, kernel_size=3,\n","                      activation='relu', padding='same',\n","                      name='conv2d_4')(x)\n","    x = layers.MaxPool2D(pool_size=2, name='maxpool2d_4')(x)\n","    x = layers.Dropout(0.1, name='dropout_4')(x)\n","\n","    # Fifth block\n","    x = layers.Conv2D(filters=192, kernel_size=3,\n","                      activation='relu', padding='same',\n","                      name='conv2d_5')(x)\n","    x = layers.MaxPool2D(pool_size=2, name='maxpool2d_5')(x)\n","    x = layers.Dropout(0.1, name='dropout_5')(x)\n","\n","    # Sixth block\n","    x = layers.Conv2D(filters=224, kernel_size=3,\n","                      activation='relu', padding='same',\n","                      name='conv2d_6')(x)\n","    x = layers.MaxPool2D(pool_size=2, name='maxpool2d_6')(x)\n","    x = layers.Dropout(0.1, name='dropout_6')(x)\n","\n","    # Seventh block\n","    x = layers.Conv2D(filters=256, kernel_size=3,\n","                      activation='relu', padding='same',\n","                      name='conv2d_7')(x)\n","    x = layers.MaxPool2D(pool_size=2, name='maxpool2d_7')(x)\n","    x = layers.Dropout(0.1, name='dropout_7')(x)\n","\n","    # Pooling and output\n","    x = layers.GlobalAveragePooling2D(name='global_average_pooling2d')(x)\n","    output = layers.Dense(units=1,\n","                          activation='sigmoid',\n","                          name='output')(x)\n","\n","    # Model creation and compilation\n","\n","    model = Model (input_layer, output)\n","    model.compile(optimizer='adam',\n","                  loss='binary_crossentropy',\n","                  metrics=['accuracy'])\n","\n","    return model"],"metadata":{"execution":{"iopub.status.busy":"2024-03-24T13:48:19.363216Z","iopub.execute_input":"2024-03-24T13:48:19.364090Z","iopub.status.idle":"2024-03-24T13:48:19.378710Z","shell.execute_reply.started":"2024-03-24T13:48:19.364058Z","shell.execute_reply":"2024-03-24T13:48:19.377525Z"},"trusted":true,"id":"rwg5Vx-4wUGq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Create a Model object\n","model = create_model()"],"metadata":{"execution":{"iopub.status.busy":"2024-03-24T13:48:26.777143Z","iopub.execute_input":"2024-03-24T13:48:26.777954Z","iopub.status.idle":"2024-03-24T13:48:26.921205Z","shell.execute_reply.started":"2024-03-24T13:48:26.777923Z","shell.execute_reply":"2024-03-24T13:48:26.920422Z"},"trusted":true,"id":"nAKWax3kwUGr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# See the layer and parameter summary\n","model.summary()"],"metadata":{"execution":{"iopub.status.busy":"2024-03-24T13:48:37.778805Z","iopub.execute_input":"2024-03-24T13:48:37.779152Z","iopub.status.idle":"2024-03-24T13:48:37.818036Z","shell.execute_reply.started":"2024-03-24T13:48:37.779127Z","shell.execute_reply":"2024-03-24T13:48:37.817113Z"},"trusted":true,"id":"-D2gByBLwUGr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from tensorflow.keras.utils import plot_model\n","\n","# See graphical representation of the model\n","plot_model(model, show_shapes=True)"],"metadata":{"execution":{"iopub.status.busy":"2024-03-24T13:48:48.395784Z","iopub.execute_input":"2024-03-24T13:48:48.396685Z","iopub.status.idle":"2024-03-24T13:48:49.015875Z","shell.execute_reply.started":"2024-03-24T13:48:48.396651Z","shell.execute_reply":"2024-03-24T13:48:49.014923Z"},"trusted":true,"id":"SdLltTbKwUGr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Another method that I've used to optimize model performance was learning rate decay. I've used exponential decay since it seemed the most promising - however, other decay schedules may show better performance.\n","\n","The *lr_decay* method is to be given to Keras' *LearningRateScheduler* object and then provided to the model as a callback."],"metadata":{"id":"xAIeRdFvwUGs"}},{"cell_type":"code","source":["# Set up exponential learning rate decay\n","def lr_decay(epoch):\n","    \"\"\"\n","    Create a learning rate reduction scheduler\n","\n","    Arguments:\n","        epoch (int): The index of the current epoch\n","\n","    Returns:\n","        lr (float): Learning rate as of epoch\n","    \"\"\"\n","\n","    initial_lr = 0.001\n","    lr = initial_lr * np.exp(-0.1 * epoch)\n","    return lr"],"metadata":{"execution":{"iopub.status.busy":"2024-03-24T13:49:03.891246Z","iopub.execute_input":"2024-03-24T13:49:03.892113Z","iopub.status.idle":"2024-03-24T13:49:03.896866Z","shell.execute_reply.started":"2024-03-24T13:49:03.892079Z","shell.execute_reply":"2024-03-24T13:49:03.895971Z"},"trusted":true,"id":"noWErh1swUGs"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Below, I create three callbacks for training - the aforementioned *LearningRateScheduler*, a *CSVLogger* object for model result saving, and a *ModelCheckpoint* object for model weight saving after each epoch. I did the last two to save results and models since I did my analysis in multiple notebook runs.\n","\n","Here, I am declaring the *CSVLogger* and *ModelCheckpoint* for demonstration - we won't be using them since I've already done the training & testing."],"metadata":{"id":"STG968_7wUGt"}},{"cell_type":"code","source":["# Import classes for metric saving, model saving, and LR reduction\n","from tensorflow.keras.callbacks import CSVLogger, ModelCheckpoint, LearningRateScheduler\n","\n","lr_scheduler = LearningRateScheduler(lr_decay, 1)\n","csv_logger = CSVLogger(filename='7-layer_double_adam_512_aug_bn_dropout01_explr.csv')\n","model_checkpoint = ModelCheckpoint(filepath='7-layer_double_adam_512_aug_bn_dropout01_explr_{epoch:04d}.keras')\n"],"metadata":{"execution":{"iopub.status.busy":"2024-03-24T13:51:49.975496Z","iopub.execute_input":"2024-03-24T13:51:49.976196Z","iopub.status.idle":"2024-03-24T13:51:49.981080Z","shell.execute_reply.started":"2024-03-24T13:51:49.976163Z","shell.execute_reply":"2024-03-24T13:51:49.980047Z"},"trusted":true,"id":"iv6o3z-NwUGt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Next, we need to define the number of training and validation steps. I've calculated them by diving sample number by batch size and then rounding the result down. Since the result is rounded down, not all images will be fed into the model. I did this consciously since I've also tested models without data augmentation - without data augmentation, I might have been providing duplicate images to the model, which I wanted to avoid."],"metadata":{"id":"QHQKRYxywUGt"}},{"cell_type":"code","source":["# Calculate the number of steps for training and validation\n","train_steps = train_gen.samples // BATCH_SIZE\n","val_steps = val_gen.samples // BATCH_SIZE"],"metadata":{"execution":{"iopub.status.busy":"2024-03-24T13:51:59.797926Z","iopub.execute_input":"2024-03-24T13:51:59.798664Z","iopub.status.idle":"2024-03-24T13:51:59.802850Z","shell.execute_reply.started":"2024-03-24T13:51:59.798633Z","shell.execute_reply":"2024-03-24T13:51:59.801849Z"},"trusted":true,"id":"DsaIaUM0wUGt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Below, I commence training will validation data and callbacks. I've commented this part out because training takes a long time and because I have the results anyway."],"metadata":{"id":"Mf-EYD2awUGt"}},{"cell_type":"code","source":["\n","history = model.fit(feed_data(train_data),\n","                    epochs=10,\n","                    steps_per_epoch=train_steps,\n","                    validation_data=(feed_data(val_data)),\n","                    validation_steps=val_steps,\n","                    shuffle=False,\n","                    callbacks=[lr_scheduler, csv_logger, model_checkpoint])"],"metadata":{"id":"f1wfxph7WhmW","execution":{"iopub.status.busy":"2024-03-24T13:53:45.042798Z","iopub.execute_input":"2024-03-24T13:53:45.043631Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now, we load the weights of the model that I think did the best among my tested architectures. It's probably not the very best possible model, but I didn't dive too deep into model tweaking. However, I did reduce overfitting."],"metadata":{"id":"gfxu-AtEwUGu"}},{"cell_type":"code","source":["# Load the weights of a pretrained model\n","model.load_weights('/kaggle/input/model-final/7-layer_adam_512_aug_dropout01_explr_0023.keras')"],"metadata":{"execution":{"iopub.status.busy":"2024-03-24T13:53:18.308980Z","iopub.execute_input":"2024-03-24T13:53:18.309340Z","iopub.status.idle":"2024-03-24T13:53:18.591062Z","shell.execute_reply.started":"2024-03-24T13:53:18.309315Z","shell.execute_reply":"2024-03-24T13:53:18.589819Z"},"trusted":true,"id":"XYQKvlC3wUGu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now, let's have a look at performance plots of all the models I've tested. These plots show how I determined that the architecture that was defined above was the best among the ones I tested."],"metadata":{"id":"dL782nw5wUGu"}},{"cell_type":"code","source":["import pandas as pd\n","one_layer = pd.read_csv(\"../input/model-final/1-layer_adam_512.csv\")\n","three_layer = pd.read_csv(\"../input/model-final/3-layer_adam_512.csv\")\n","five_layer = pd.read_csv(\"../input/model-final/5-layer_adam_512.csv\")\n","five_layer_aug = pd.read_csv(\"../input/model-final/5-layer_adam_512_aug.csv\")\n","seven_layer = pd.read_csv(\"../input/model-final/7-layer_adam_512.csv\")\n","\n","\n","# Augmented\n","# I have two separate CSV files since I first trained the model for 20 epochs and decided to add another 10 later.\n","seven_layer_aug20 = pd.read_csv(\"../input/model-final/7-layer_adam_512_aug.csv\")\n","seven_layer_aug30 = pd.read_csv(\"../input/model-final/7-layer_adam_512_aug (1).csv\")\n","\n","# Joining the two CSV files\n","seven_layer_aug = seven_layer_aug20.append(seven_layer_aug30, ignore_index=True)\n","\n","# Dropout & LR Reduction\n","seven_layer_aug_drop_20 = pd.read_csv(\"../input/model-final/7-layer_adam_512_aug_dropout02.csv\")\n","seven_layer_aug_drop_10 = pd.read_csv(\"../input/model-final/7-layer_adam_512_aug_dropout01.csv\")\n","seven_layer_aug_drop_10_explr = pd.read_csv(\"../input/model-final/7-layer_adam_512_aug_dropout01_explr.csv\")"],"metadata":{"trusted":true,"id":"V9JfGPCKwUGu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def plot_loss(results, title, ylim=None, figsize=(15, 15)):\n","    \"\"\"\n","    Plot the loss metrics from a DataFrame file\n","\n","    Arguments:\n","        results (pd.DataFrame): DataFrame containing loss metrics\n","        title (string): Title for the plot\n","        ylim (float): Limit for the plot's y axis, default=None\n","        figsize (tuple of ints): Figure size, default=(15, 15)\n","    \"\"\"\n","\n","    plt.figure(figsize=figsize)\n","\n","    for name, result in results:\n","        val = plt.plot(range(len((result['epoch']))), result['val_loss'],\n","                       '--', label=name.title()+', Validation', lw=3.0)\n","        plt.plot(range(len((result['epoch']))), result['loss'], color=val[0].get_color(),\n","                 label=name.title()+', Training', lw=3.0)\n","\n","    plt.title(title)\n","    plt.xlabel('Epochs')\n","    plt.ylabel('Loss')\n","    plt.legend()\n","\n","    plt.ylim(ylim)\n","    plt.grid(lw=2, ls='--')"],"metadata":{"trusted":true,"id":"TNowrp_AwUGu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def plot_accuracy(results, title, x_range=20, figsize=(15, 15)):\n","    \"\"\"\n","    Plot the accuracy metrics from a DataFrame file\n","\n","    Arguments:\n","        results (pd.DataFrame): DataFrame containing accuracy metrics\n","        title (string): Title for the plot\n","        ylim (float): Limit for the plot's y axis, default=None\n","        figsize (tuple of ints): Figure size, default=(15, 15)\n","    \"\"\"\n","\n","    plt.figure(figsize=figsize)\n","\n","    for name, result in results:\n","        val = plt.plot(range(len((result['epoch']))), result['val_accuracy'],\n","                       '--', label=name.title()+', Validation', lw=3.0)\n","        plt.plot(range(len((result['epoch']))), result['accuracy'], color=val[0].get_color(),\n","                 label=name.title()+', Training', lw=3.0)\n","\n","    plt.title(title)\n","    plt.xlabel('Epochs')\n","    plt.ylabel('Accuracy')\n","    plt.legend()\n","    plt.grid(lw=2, ls='--')"],"metadata":{"trusted":true,"id":"HqtUUQANwUGu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plot_loss([('3 Layers',  three_layer),\n","           ('5 Layers', five_layer),\n","           ('7 Layers', seven_layer)],\n","          'Loss, 3, 5, & 7 Conv Layers')"],"metadata":{"trusted":true,"id":"hFpLyb-ewUGv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plot_accuracy([('3 Layers',  three_layer),\n","               ('5 Layers', five_layer),\n","               ('7 Layers', seven_layer)],\n","              'Accuracy, 3, 5, & 7 Conv Layers')"],"metadata":{"trusted":true,"id":"MqPdC5uSwUGv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plot_loss([('Without Augmentation',  seven_layer),\n","           ('With Augmentation',  seven_layer_aug)],\n","          'Loss, 7 Conv Layers W/ & W/out Image Augmentation')"],"metadata":{"trusted":true,"id":"n6uct7m8wUGv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plot_accuracy([('Without Augmentation',  seven_layer),\n","               ('With Augmentation',  seven_layer_aug)],\n","              'Accuracy, 7 Conv Layers W/ & W/out Image Augmentation')"],"metadata":{"trusted":true,"id":"3RDQqET4wUGv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plot_loss([('0.2 Dropout',  seven_layer_aug_drop_20),\n","           ('0.1 Dropout',  seven_layer_aug_drop_10)],\n","          'Loss, Regularized Model W/ & W/out Dropout')"],"metadata":{"trusted":true,"id":"f0QkQz1NwUGv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plot_accuracy([('0.2 Dropout',  seven_layer_aug_drop_20),\n","               ('0.1 Dropout',  seven_layer_aug_drop_10)],\n","              'Accuracy, Regularized Model W/ & W/out Dropout')"],"metadata":{"trusted":true,"id":"f9Ub3UL_wUGv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plot_loss([('Without Decay',  seven_layer_aug_drop_10),\n","           ('With Decay',  seven_layer_aug_drop_10_explr)],\n","          'Loss, Regularized Model W/ & W/out LR Decay')"],"metadata":{"trusted":true,"id":"QhN_RyAswUGv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plot_accuracy([('Without Decay',  seven_layer_aug_drop_10),\n","               ('With Decay',  seven_layer_aug_drop_10_explr)],\n","              'Accuracy, Regularized Model W/ & W/out LR Decay')"],"metadata":{"trusted":true,"id":"DFjQRhkkwUGw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plot_loss([('Base',  seven_layer),\n","           ('Regularized',  seven_layer_aug_drop_10_explr)],\n","          'Loss, 7-Layer Base & Regularized')"],"metadata":{"trusted":true,"id":"pj9uMAL9wUGw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plot_accuracy([('Base',  seven_layer),\n","               ('Regularized',  seven_layer_aug_drop_10_explr)],\n","              'Accuracy, 7-Layer Base & Regularized')"],"metadata":{"trusted":true,"id":"HTF754enwUGw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Let's see how the model does on test data. I've used the test data that I've declared earlier, as well as checked test data with augmentation just out of interest. I also computed precision and recall for both augmented and base test data.\n","\n","Test results with and without data augmentation are pretty close, so the model did a good job of learning the distribution of the data."],"metadata":{"id":"udLr_w9LwUGw"}},{"cell_type":"code","source":["# Evaluate the model on the test set\n","test_steps = test_gen.samples // BATCH_SIZE\n","model.evaluate(test_data, steps=test_steps)\n","\n","# Declare an image generator on test data for augmented images\n","test_aug_gen = datagen.flow_from_dataframe(test_df,\n","                                           target_size=(512, 512),\n","                                           color_mode='grayscale',\n","                                           batch_size=BATCH_SIZE,\n","                                           class_mode='binary',\n","                                           shuffle=False)\n","\n","test_aug_data = tf.data.Dataset.from_generator(lambda: test_aug_gen,\n","                                           output_types=(tf.float32, tf.int32),\n","                                           output_shapes=([None, 512, 512, 1], [None, ]))\n","\n","# Evaluate the model on augmented test data\n","test_aug_steps = test_aug_gen.samples // BATCH_SIZE\n","model.evaluate(test_aug_data, steps=test_aug_steps)"],"metadata":{"trusted":true,"id":"mXEBYshFwUGx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Calculate precision and recall based on  test data\n","\n","precision = tf.keras.metrics.Precision()\n","recall = tf.keras.metrics.Recall()\n","\n","predictions = model.predict(test_data, steps=test_steps).flatten()\n","y_true = test_gen.classes[:len(predictions)]\n","\n","precision.update_state(y_true, predictions)\n","recall.update_state(y_true, predictions)"],"metadata":{"trusted":true,"id":"P6g-WVfJwUGx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print('Precision with base test data:', precision.result().numpy())\n","print('Recall with base test data:', recall.result().numpy())"],"metadata":{"trusted":true,"id":"LTYnRLwawUGx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Calculate precision and recall based on augmented test data\n","\n","precision.reset_states()\n","recall.reset_states()\n","\n","predictions_aug = model.predict(test_aug_data, steps=test_steps).flatten()\n","y_true = test_gen.classes[:len(predictions_aug)]\n","\n","precision.update_state(y_true, predictions_aug)\n","recall.update_state(y_true, predictions_aug)"],"metadata":{"trusted":true,"id":"eAiQPjVxwUGx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print('Precision with augmented test data:', precision.result().numpy())\n","print('Recall with augmented test data:', recall.result().numpy())"],"metadata":{"trusted":true,"id":"pIg8cWpowUGx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["And a confusion matrix for a better look at true and false prediction counts."],"metadata":{"id":"xvobMqMewUGx"}},{"cell_type":"code","source":["from sklearn.metrics import confusion_matrix\n","\n","confusion_mat = confusion_matrix(y_true, predictions > 0.5)"],"metadata":{"trusted":true,"id":"KZVISIBmwUGy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["fig, ax = plt.subplots()\n","\n","ax.matshow(confusion_mat, cmap=plt.cm.Oranges)\n","\n","ax.set_xlabel('Prediction')\n","ax.set_ylabel('True Label')\n","\n","tick_labels = ['Normal', 'Pneumonia']\n","\n","ax.set_xticks(range(len(tick_labels)))\n","ax.set_yticks(range(len(tick_labels)))\n","ax.set_xticklabels(tick_labels)\n","ax.set_yticklabels(tick_labels)\n","\n","for i in range(len(tick_labels)):\n","    for j in range(len(tick_labels)):\n","        ax.text(j, i, confusion_mat[i, j],\n","               ha='center', va='center')\n","\n","plt.show()"],"metadata":{"trusted":true,"id":"KBY42-WmwUGy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["To give you an idea of how the model performs, let's also plot a few images along with their predictions and true labels."],"metadata":{"id":"X2F8EaW9wUGy"}},{"cell_type":"code","source":["from tensorflow.keras.preprocessing.image import array_to_img\n","\n","def plot_image(image, prediction, label):\n","    \"\"\"\n","    Plot images along with predictions and true labels\n","\n","    Arguments:\n","        image (4-D array): The image to visualize\n","        prediction (2-D array): Model's prediction on the image\n","        label (1-D array): True label of the image\n","    \"\"\"\n","\n","    label_names = ['Normal', 'Pneumonia']\n","\n","    plt.grid(False)\n","    plt.xticks([])\n","    plt.yticks([])\n","\n","    plt.imshow(array_to_img(image * 255), interpolation='nearest', cmap='gray')\n","\n","    if prediction <= 0.5:\n","        predicted_label = 0\n","    else:\n","        predicted_label = 1\n","\n","    if predicted_label == label:\n","        color = 'blue'\n","    else:\n","        color = 'red'\n","\n","    plt.xlabel(\"{} {:2.0f}% \\n ({})\".format(label_names[predicted_label], 100 * prediction[0], label_names[label]), color=color)"],"metadata":{"id":"ZfmjSDyPuSbt","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["images, labels = next(iter(test_data))\n","y_pred = model.predict(images)\n","num_rows = 4\n","num_cols = 4\n","num_images = num_rows * num_cols\n","plt.figure(figsize=(2*2*num_cols, 2.2*2*num_rows))\n","\n","for i in range(num_images):\n","    plt.subplot(num_rows, num_cols, i+1)\n","    plot_image(images[i], y_pred[i], labels[i])\n","plt.show()"],"metadata":{"id":"qHQ_Q2R8vjdu","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Above, I've used 0.5 as the decision threshold. This threshold is also used during training for accuracy computations.\n","\n","You can adjust the decision threshold in the [0, 1] range to tweak the confidence with which the model will make predictions. Needless to say, this will change how well the model performs. For a better look, let's see how the metrics change with a few thresholds values."],"metadata":{"id":"JQup1QBVwUGy"}},{"cell_type":"code","source":["def calculate_metrics(predictions, labels, threshold):\n","    \"\"\"\n","    Calculate precision, recall, and binary accuracy given a dataset\n","\n","    Arguments:\n","        predictions (1-D array): Model predictions\n","        labels (1-D array): True labels\n","        threshold (float): The metrics' decision threshold\n","\n","    Returns:\n","        Mean of metrics on all batches of data\n","    \"\"\"\n","\n","    precision = tf.keras.metrics.Precision(thresholds=threshold)\n","    recall = tf.keras.metrics.Recall(thresholds=threshold)\n","    binary_accuracy = tf.keras.metrics.BinaryAccuracy(threshold=threshold)\n","\n","    precision.update_state(y_true, predictions)\n","    recall.update_state(y_true, predictions)\n","    binary_accuracy.update_state(y_true, predictions)\n","\n","    return precision.result().numpy(), recall.result().numpy(), binary_accuracy.result().numpy()"],"metadata":{"trusted":true,"id":"y_GnjReAwUGz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["thresholds = np.linspace(0, 1, 100)\n","\n","precisions, recalls, binary_accuracies = [], [], []"],"metadata":{"trusted":true,"id":"cDG-nI6ewUGz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for threshold in thresholds:\n","    precision, recall, binary_accuracy = calculate_metrics(predictions, y_true, threshold)\n","\n","    precisions.append(precision)\n","    recalls.append(recall)\n","    binary_accuracies.append(binary_accuracy)"],"metadata":{"trusted":true,"id":"XAKa_UNOwUGz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def plot_metrics(precisions, recalls, accuracies, thresholds):\n","    \"\"\"\n","    Plot the dependency of metrics on threshold values\n","\n","    Arguments:\n","        precisions (1-D array): Precisions calculated on thresholds\n","        recalls (1-D array): Recalls calculated on thresholds\n","        accuraries (1-D array): Accuracies calculated on thresholds\n","        thresholds (1-D array): Thresholds upon which metrics were evaluated\n","\n","    \"\"\"\n","\n","    plt.plot(thresholds, precisions, label='Precison')\n","    plt.plot(thresholds, recalls, '--', label='Recall')\n","    plt.plot(thresholds, accuracies,'-.', label='Accuracy')\n","\n","    plt.xticks(np.linspace(0, 1, 10))\n","    plt.xlabel('Threshold')\n","    plt.ylabel('Score')\n","    plt.ylim([0.7, 1])\n","    plt.grid()\n","    plt.legend()\n","    plt.title('The Dependence Of Metric Scores On The Decision Threshold')\n","    plt.show()"],"metadata":{"trusted":true,"id":"HSoYEkGpwUGz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plot_metrics(precisions, recalls, binary_accuracies, thresholds)"],"metadata":{"trusted":true,"id":"xtwR8OozwUG0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["As we can see, precision is low with low thresholds and high with high thresholds. The dependence for recall is inverse - it's the highest when the threshold is 0 and the lowest when it's 1. As for accuracy, it's somewhat dependent on precision and recall, so its highest scores are somewhere in the middle."],"metadata":{"id":"dfY_0dESwUG0"}},{"cell_type":"markdown","source":["And finally, let's have a look at saliency maps and Grad-CAMs."],"metadata":{"id":"3Esv7wzMwUG0"}},{"cell_type":"code","source":["pip install https://github.com/raghakot/keras-vis/archive/master.zip"],"metadata":{"trusted":true,"id":"_5o6I56EwUG0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from vis.visualization import visualize_cam, visualize_saliency, visualize_activation, overlay\n","from vis.utils import utils\n","\n","images, labels = next(iter(test_data))\n","\n","predictions = model.predict(images)"],"metadata":{"trusted":true,"id":"JbrBOMtjwUG0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def visualize_cams(images, predictions, labels):\n","    '''\n","    Plot Grad-CAMs on a batch of images\n","\n","    Arguments:\n","        images (4-D array): Images to compute Grad-CAM on\n","        predictions (2-D array): Predicted labels\n","        labels (1-D array): True labels\n","    '''\n","\n","    rows, cols = 3, 3\n","\n","    fig, axes = plt.subplots(rows, cols)\n","    fig.set_size_inches(2*2*cols, 2.2*2*rows)\n","    fig.suptitle('Grad-CAMs On 9 Images')\n","    current_index = 0\n","    label_names = ['Normal', 'Pneumonia']\n","\n","    for i in range(rows) :\n","        for j in range(cols):\n","            image = images[current_index]\n","            image_rgb = tf.image.grayscale_to_rgb(image)\n","            visualization = visualize_cam(model, -1, filter_indices=0, seed_input=image, penultimate_layer_idx=-4)\n","\n","            axes[i, j].imshow(tf.image.grayscale_to_rgb(image))\n","            axes[i, j].imshow(visualization, interpolation='nearest', alpha=0.6)\n","            axes[i, j].set_xticks([])\n","            axes[i, j].set_yticks([])\n","\n","            if predictions[current_index] <= 0.5:\n","                predicted_label = 0\n","            else:\n","                predicted_label = 1\n","\n","            if predicted_label == labels[current_index]:\n","                color = 'blue'\n","            else:\n","                color = 'red'\n","\n","            label = labels[current_index]\n","            axes[i, j].set_xlabel(\"{} {:2.0f}% \\n ({})\".format(label_names[predicted_label], 100 * predictions[current_index][0], label_names[label]), color=color)\n","            current_index += 1\n","\n","    plt.show()"],"metadata":{"trusted":true,"id":"crx04ZTIwUG0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Visualize Grad-CAMs\n","visualize_cams(images, predictions, labels)"],"metadata":{"trusted":true,"id":"VJmGCOyywUG1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def visualize_sal(images, predictions, labels):\n","    \"\"\"\n","    Plot saliency map on a batch of images\n","\n","    Arguments:\n","        images (array-like): Images to compute saliency upon\n","        predictions (array-like): Model's predictions on images\n","        labels (array-like): True labels of the images\n","    \"\"\"\n","\n","    rows, cols = 3, 3\n","\n","    fig, axes = plt.subplots(rows, cols)\n","    fig.set_size_inches(2*2*cols, 2.2*2*rows)\n","    fig.suptitle('Saliency Maps On 9 Images')\n","    current_index = 0\n","    label_names = ['Normal', 'Pneumonia']\n","\n","    for i in range(rows) :\n","        for j in range(cols):\n","            image = images[current_index]\n","            image_rgb = tf.image.grayscale_to_rgb(image)\n","            visualization = visualize_saliency(model, -1, filter_indices=None, seed_input=image)\n","\n","            #axes[i, j].imshow(tf.image.grayscale_to_rgb(image))\n","            axes[i, j].imshow(visualization * 255, interpolation='nearest', alpha=1)\n","            axes[i, j].set_xticks([])\n","            axes[i, j].set_yticks([])\n","\n","            if predictions[current_index] <= 0.5:\n","                predicted_label = 0\n","            else:\n","                predicted_label = 1\n","\n","            if predicted_label == labels[current_index]:\n","                color = 'blue'\n","            else:\n","                color = 'red'\n","\n","            label = labels[current_index]\n","            axes[i, j].set_xlabel(\"{} {:2.0f}% \\n ({})\".format(label_names[predicted_label], 100 * predictions[current_index][0], label_names[label]), color=color)\n","            current_index += 1"],"metadata":{"trusted":true,"id":"FaysTS5JwUG1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Visualize saliency maps\n","visualize_sal(images, predictions, labels)"],"metadata":{"trusted":true,"id":"dAfQMUd6wUG1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Based on saliency maps and Grad-CAMs, the model seems to pay attention to the right areas on the images (i.e. the lungs). However, the 1st and 9th Grad-CAMs don't look too good - the upper left corner is excessively highlighted in them. This may be because the model needs additional tweaking, or maybe Grad-CAM failed to localize regions of interest with those particular images. On saliency maps, these images look fine (although the saliency maps are very vague as well).\n","\n","For real-world deployment, models like this would require more in-depth analysis and tweaking. Furthemore, cooperation between ML engineers and doctors would be necessary to identify weak points & faulty predictions and then try to fix the model.\n","\n","With all that said, this experiment has been pretty interesting, and hopefully, you've learned a thing or two from it (particularly regarding the *ImageDataGenerator* class and overfitting reduction - I certainly did)."],"metadata":{"id":"bR97wtdVwUG1"}}]}